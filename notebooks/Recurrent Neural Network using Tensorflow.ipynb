{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network\n",
    "\n",
    "-----\n",
    "\n",
    "In this notebook, I would be trying to recreate an example for learning Recurrent Neural Networks using LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "num_epochs = 100\n",
    "total_series_length = 50000\n",
    "truncated_backprop_length = 15\n",
    "state_size = 4\n",
    "num_classes = 2\n",
    "echo_step = 3\n",
    "batch_size = 5\n",
    "num_batches = total_series_length//batch_size//truncated_backprop_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateData():\n",
    "    x = np.array(np.random.choice(2, total_series_length, p=[0.5, 0.5]))\n",
    "    y = np.roll(x, echo_step)\n",
    "    y[0:echo_step] = 0\n",
    "    \n",
    "    x = x.reshape((batch_size, -1))\n",
    "    y = y.reshape((batch_size, -1))\n",
    "    \n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batchX_placeholder = tf.placeholder(tf.float32, [batch_size, truncated_backprop_length])\n",
    "batchY_placeholder = tf.placeholder(tf.int32, [batch_size, truncated_backprop_length])\n",
    "\n",
    "init_state = tf.placeholder(tf.float32, [batch_size, state_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(np.random.rand(state_size+1, state_size), dtype=tf.float32)\n",
    "b = tf.Variable(np.zeros((1, state_size)), dtype=tf.float32)\n",
    "\n",
    "W2 = tf.Variable(np.random.rand(state_size, num_classes), dtype=tf.float32)\n",
    "b2 = tf.Variable(np.zeros((1, num_classes)), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs_series = tf.unstack(batchX_placeholder, axis=1)\n",
    "labels_series = tf.unstack(batchY_placeholder, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forward Pass\n",
    "current_state = init_state\n",
    "states_series = []\n",
    "\n",
    "for current_input in inputs_series:\n",
    "    current_input = tf.reshape(current_input, [batch_size, 1])\n",
    "    input_and_state_concatenated = tf.concat([current_input, current_state], 1)\n",
    "    \n",
    "    next_state = tf.tanh(tf.matmul(input_and_state_concatenated, W) + b)\n",
    "    states_series.append(next_state)\n",
    "    current_state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_series = [tf.matmul(state, W2) + b2 for state in states_series] \n",
    "predictions_series = [tf.nn.softmax(logits) for logits in logits_series]\n",
    "\n",
    "losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels) for logits, labels in zip(logits_series,labels_series)]\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "\n",
    "train_step = tf.train.AdagradOptimizer(0.3).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(loss_list, predictions_series, batchX, batchY):\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.cla()\n",
    "    plt.plot(loss_list)\n",
    "\n",
    "    for batch_series_idx in range(5):\n",
    "        one_hot_output_series = np.array(predictions_series)[:, batch_series_idx, :]\n",
    "        single_output_series = np.array([(1 if out[0] < 0.5 else 0) for out in one_hot_output_series])\n",
    "\n",
    "        plt.subplot(2, 3, batch_series_idx + 2)\n",
    "        plt.cla()\n",
    "        plt.axis([0, truncated_backprop_length, 0, 2])\n",
    "        left_offset = range(truncated_backprop_length)\n",
    "        plt.bar(left_offset, batchX[batch_series_idx, :], width=1, color=\"blue\")\n",
    "        plt.bar(left_offset, batchY[batch_series_idx, :] * 0.5, width=1, color=\"red\")\n",
    "        plt.bar(left_offset, single_output_series * 0.3, width=1, color=\"green\")\n",
    "\n",
    "    plt.draw()\n",
    "    plt.pause(0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/fjcamillo/anaconda2/envs/py3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5490ea4f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Data, epoch 0\n",
      "Step: 0, Loss: 0.6982260346412659\n",
      "Step: 100, Loss: 0.6909540891647339\n",
      "Step: 200, Loss: 0.6933677792549133\n",
      "Step: 300, Loss: 0.686427891254425\n",
      "Step: 400, Loss: 0.64141446352005\n",
      "Step: 500, Loss: 0.24943743646144867\n",
      "Step: 600, Loss: 0.3418334126472473\n",
      "New Data, epoch 1\n",
      "Step: 0, Loss: 0.22910964488983154\n",
      "Step: 100, Loss: 0.2557568848133087\n",
      "Step: 200, Loss: 0.3126926124095917\n",
      "Step: 300, Loss: 0.30893421173095703\n",
      "Step: 400, Loss: 0.3965648114681244\n",
      "Step: 500, Loss: 0.38906967639923096\n",
      "Step: 600, Loss: 0.3283645808696747\n",
      "New Data, epoch 2\n",
      "Step: 0, Loss: 0.2646329998970032\n",
      "Step: 100, Loss: 0.2883141040802002\n",
      "Step: 200, Loss: 0.2185949981212616\n",
      "Step: 300, Loss: 0.25265076756477356\n",
      "Step: 400, Loss: 0.33784013986587524\n",
      "Step: 500, Loss: 0.27398058772087097\n",
      "Step: 600, Loss: 0.42252618074417114\n",
      "New Data, epoch 3\n",
      "Step: 0, Loss: 0.22057533264160156\n",
      "Step: 100, Loss: 0.37906351685523987\n",
      "Step: 200, Loss: 0.34196794033050537\n",
      "Step: 300, Loss: 0.2647395133972168\n",
      "Step: 400, Loss: 0.3427656590938568\n",
      "Step: 500, Loss: 0.34952452778816223\n",
      "Step: 600, Loss: 0.309166818857193\n",
      "New Data, epoch 4\n",
      "Step: 0, Loss: 0.24491818249225616\n",
      "Step: 100, Loss: 0.3695071041584015\n",
      "Step: 200, Loss: 0.31686753034591675\n",
      "Step: 300, Loss: 0.40149542689323425\n",
      "Step: 400, Loss: 0.34123411774635315\n",
      "Step: 500, Loss: 0.3129183053970337\n",
      "Step: 600, Loss: 0.397793710231781\n",
      "New Data, epoch 5\n",
      "Step: 0, Loss: 0.24626217782497406\n",
      "Step: 100, Loss: 0.31774476170539856\n",
      "Step: 200, Loss: 0.33931034803390503\n",
      "Step: 300, Loss: 0.36961907148361206\n",
      "Step: 400, Loss: 0.3067168891429901\n",
      "Step: 500, Loss: 0.31204837560653687\n",
      "Step: 600, Loss: 0.30321648716926575\n",
      "New Data, epoch 6\n",
      "Step: 0, Loss: 0.22025655210018158\n",
      "Step: 100, Loss: 0.35880857706069946\n",
      "Step: 200, Loss: 0.3688931167125702\n",
      "Step: 300, Loss: 0.2805851101875305\n",
      "Step: 400, Loss: 0.43266743421554565\n",
      "Step: 500, Loss: 0.2813248634338379\n",
      "Step: 600, Loss: 0.25106188654899597\n",
      "New Data, epoch 7\n",
      "Step: 0, Loss: 0.2504505515098572\n",
      "Step: 100, Loss: 0.3656901717185974\n",
      "Step: 200, Loss: 0.24938532710075378\n",
      "Step: 300, Loss: 0.3431476652622223\n",
      "Step: 400, Loss: 0.3381426930427551\n",
      "Step: 500, Loss: 0.42689061164855957\n",
      "Step: 600, Loss: 0.31268879771232605\n",
      "New Data, epoch 8\n",
      "Step: 0, Loss: 0.20229299366474152\n",
      "Step: 100, Loss: 0.30856025218963623\n",
      "Step: 200, Loss: 0.40106141567230225\n",
      "Step: 300, Loss: 0.33974289894104004\n",
      "Step: 400, Loss: 0.4012896418571472\n",
      "Step: 500, Loss: 0.36647748947143555\n",
      "Step: 600, Loss: 0.19822929799556732\n",
      "New Data, epoch 9\n",
      "Step: 0, Loss: 0.23477788269519806\n",
      "Step: 100, Loss: 0.31160709261894226\n",
      "Step: 200, Loss: 0.3957231342792511\n",
      "Step: 300, Loss: 0.37052738666534424\n",
      "Step: 400, Loss: 0.335122287273407\n",
      "Step: 500, Loss: 0.34276434779167175\n",
      "Step: 600, Loss: 0.3113502562046051\n",
      "New Data, epoch 10\n",
      "Step: 0, Loss: 0.23750442266464233\n",
      "Step: 100, Loss: 0.2780710756778717\n",
      "Step: 200, Loss: 0.3402448296546936\n",
      "Step: 300, Loss: 0.2143513560295105\n",
      "Step: 400, Loss: 0.21956700086593628\n",
      "Step: 500, Loss: 0.30616235733032227\n",
      "Step: 600, Loss: 0.28270331025123596\n",
      "New Data, epoch 11\n",
      "Step: 0, Loss: 0.25087955594062805\n",
      "Step: 100, Loss: 0.3748871386051178\n",
      "Step: 200, Loss: 0.34219980239868164\n",
      "Step: 300, Loss: 0.3078553378582001\n",
      "Step: 400, Loss: 0.2804172933101654\n",
      "Step: 500, Loss: 0.39509478211402893\n",
      "Step: 600, Loss: 0.3825991451740265\n",
      "New Data, epoch 12\n",
      "Step: 0, Loss: 0.25700533390045166\n",
      "Step: 100, Loss: 0.30862191319465637\n",
      "Step: 200, Loss: 0.3373582363128662\n",
      "Step: 300, Loss: 0.3391311764717102\n",
      "Step: 400, Loss: 0.33483651280403137\n",
      "Step: 500, Loss: 0.25842711329460144\n",
      "Step: 600, Loss: 0.36631453037261963\n",
      "New Data, epoch 13\n",
      "Step: 0, Loss: 0.2076188027858734\n",
      "Step: 100, Loss: 0.30588120222091675\n",
      "Step: 200, Loss: 0.2841438949108124\n",
      "Step: 300, Loss: 0.22148168087005615\n",
      "Step: 400, Loss: 0.3366895914077759\n",
      "Step: 500, Loss: 0.31241077184677124\n",
      "Step: 600, Loss: 0.3747667670249939\n",
      "New Data, epoch 14\n",
      "Step: 0, Loss: 0.228315070271492\n",
      "Step: 100, Loss: 0.25816231966018677\n",
      "Step: 200, Loss: 0.2824461758136749\n",
      "Step: 300, Loss: 0.22505350410938263\n",
      "Step: 400, Loss: 0.2216653972864151\n",
      "Step: 500, Loss: 0.3700339198112488\n",
      "Step: 600, Loss: 0.27966228127479553\n",
      "New Data, epoch 15\n",
      "Step: 0, Loss: 0.24261488020420074\n",
      "Step: 100, Loss: 0.24835042655467987\n",
      "Step: 200, Loss: 0.21833577752113342\n",
      "Step: 300, Loss: 0.367362380027771\n",
      "Step: 400, Loss: 0.2844788432121277\n",
      "Step: 500, Loss: 0.33376091718673706\n",
      "Step: 600, Loss: 0.27628982067108154\n",
      "New Data, epoch 16\n",
      "Step: 0, Loss: 0.22360974550247192\n",
      "Step: 100, Loss: 0.3136475384235382\n",
      "Step: 200, Loss: 0.2515733242034912\n",
      "Step: 300, Loss: 0.30556610226631165\n",
      "Step: 400, Loss: 0.3123950660228729\n",
      "Step: 500, Loss: 0.3647865355014801\n",
      "Step: 600, Loss: 0.30471551418304443\n",
      "New Data, epoch 17\n",
      "Step: 0, Loss: 0.22100120782852173\n",
      "Step: 100, Loss: 0.3143523335456848\n",
      "Step: 200, Loss: 0.3608733117580414\n",
      "Step: 300, Loss: 0.33087000250816345\n",
      "Step: 400, Loss: 0.3392517566680908\n",
      "Step: 500, Loss: 0.24596166610717773\n",
      "Step: 600, Loss: 0.3073767125606537\n",
      "New Data, epoch 18\n",
      "Step: 0, Loss: 0.22158871591091156\n",
      "Step: 100, Loss: 0.3092437982559204\n",
      "Step: 200, Loss: 0.3750339448451996\n",
      "Step: 300, Loss: 0.3132021725177765\n",
      "Step: 400, Loss: 0.34019213914871216\n",
      "Step: 500, Loss: 0.41638660430908203\n",
      "Step: 600, Loss: 0.3617629110813141\n",
      "New Data, epoch 19\n",
      "Step: 0, Loss: 0.25157737731933594\n",
      "Step: 100, Loss: 0.310151070356369\n",
      "Step: 200, Loss: 0.3375006318092346\n",
      "Step: 300, Loss: 0.28011348843574524\n",
      "Step: 400, Loss: 0.2810402810573578\n",
      "Step: 500, Loss: 0.34084466099739075\n",
      "Step: 600, Loss: 0.2787906527519226\n",
      "New Data, epoch 20\n",
      "Step: 0, Loss: 0.2441752552986145\n",
      "Step: 100, Loss: 0.28029096126556396\n",
      "Step: 200, Loss: 0.3077937364578247\n",
      "Step: 300, Loss: 0.36614444851875305\n",
      "Step: 400, Loss: 0.31253165006637573\n",
      "Step: 500, Loss: 0.309968501329422\n",
      "Step: 600, Loss: 0.30967962741851807\n",
      "New Data, epoch 21\n",
      "Step: 0, Loss: 0.246532142162323\n",
      "Step: 100, Loss: 0.22613424062728882\n",
      "Step: 200, Loss: 0.37034526467323303\n",
      "Step: 300, Loss: 0.3445335924625397\n",
      "Step: 400, Loss: 0.42420947551727295\n",
      "Step: 500, Loss: 0.3654272258281708\n",
      "Step: 600, Loss: 0.25328540802001953\n",
      "New Data, epoch 22\n",
      "Step: 0, Loss: 0.22240425646305084\n",
      "Step: 100, Loss: 0.3122937083244324\n",
      "Step: 200, Loss: 0.3450770676136017\n",
      "Step: 300, Loss: 0.21679405868053436\n",
      "Step: 400, Loss: 0.42447808384895325\n",
      "Step: 500, Loss: 0.3435874283313751\n",
      "Step: 600, Loss: 0.2828139066696167\n",
      "New Data, epoch 23\n",
      "Step: 0, Loss: 0.26869848370552063\n",
      "Step: 100, Loss: 0.30798351764678955\n",
      "Step: 200, Loss: 0.4266171455383301\n",
      "Step: 300, Loss: 0.37276163697242737\n",
      "Step: 400, Loss: 0.2521352171897888\n",
      "Step: 500, Loss: 0.36709651350975037\n",
      "Step: 600, Loss: 0.3706493079662323\n",
      "New Data, epoch 24\n",
      "Step: 0, Loss: 0.26523151993751526\n",
      "Step: 100, Loss: 0.21944518387317657\n",
      "Step: 200, Loss: 0.36691322922706604\n",
      "Step: 300, Loss: 0.36727848649024963\n",
      "Step: 400, Loss: 0.28422003984451294\n",
      "Step: 500, Loss: 0.33901217579841614\n",
      "Step: 600, Loss: 0.27862027287483215\n",
      "New Data, epoch 25\n",
      "Step: 0, Loss: 0.25472840666770935\n",
      "Step: 100, Loss: 0.3091520071029663\n",
      "Step: 200, Loss: 0.30637475848197937\n",
      "Step: 300, Loss: 0.2825799584388733\n",
      "Step: 400, Loss: 0.3407922387123108\n",
      "Step: 500, Loss: 0.21667246520519257\n",
      "Step: 600, Loss: 0.33685606718063354\n",
      "New Data, epoch 26\n",
      "Step: 0, Loss: 0.22962529957294464\n",
      "Step: 100, Loss: 0.3128781020641327\n",
      "Step: 200, Loss: 0.2225482165813446\n",
      "Step: 300, Loss: 0.2814467251300812\n",
      "Step: 400, Loss: 0.34204891324043274\n",
      "Step: 500, Loss: 0.37291207909584045\n",
      "Step: 600, Loss: 0.3704758584499359\n",
      "New Data, epoch 27\n",
      "Step: 0, Loss: 0.22577865421772003\n",
      "Step: 100, Loss: 0.30978041887283325\n",
      "Step: 200, Loss: 0.3109268844127655\n",
      "Step: 300, Loss: 0.3680381178855896\n",
      "Step: 400, Loss: 0.3696677088737488\n",
      "Step: 500, Loss: 0.31171536445617676\n",
      "Step: 600, Loss: 0.247725248336792\n",
      "New Data, epoch 28\n",
      "Step: 0, Loss: 0.2259649932384491\n",
      "Step: 100, Loss: 0.3093257248401642\n",
      "Step: 200, Loss: 0.3960622251033783\n",
      "Step: 300, Loss: 0.395999938249588\n",
      "Step: 400, Loss: 0.3093491494655609\n",
      "Step: 500, Loss: 0.27928057312965393\n",
      "Step: 600, Loss: 0.3434317409992218\n",
      "New Data, epoch 29\n",
      "Step: 0, Loss: 0.24196714162826538\n",
      "Step: 100, Loss: 0.34058743715286255\n",
      "Step: 200, Loss: 0.27742257714271545\n",
      "Step: 300, Loss: 0.3999897837638855\n",
      "Step: 400, Loss: 0.3668802082538605\n",
      "Step: 500, Loss: 0.3401950001716614\n",
      "Step: 600, Loss: 0.3116849958896637\n",
      "New Data, epoch 30\n",
      "Step: 0, Loss: 0.2509762942790985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100, Loss: 0.3426566421985626\n",
      "Step: 200, Loss: 0.30971381068229675\n",
      "Step: 300, Loss: 0.36648499965667725\n",
      "Step: 400, Loss: 0.48000189661979675\n",
      "Step: 500, Loss: 0.40259334444999695\n",
      "Step: 600, Loss: 0.3388187885284424\n",
      "New Data, epoch 31\n",
      "Step: 0, Loss: 0.2548675537109375\n",
      "Step: 100, Loss: 0.3991827666759491\n",
      "Step: 200, Loss: 0.30854177474975586\n",
      "Step: 300, Loss: 0.3764662742614746\n",
      "Step: 400, Loss: 0.3099783658981323\n",
      "Step: 500, Loss: 0.288003146648407\n",
      "Step: 600, Loss: 0.3098949193954468\n",
      "New Data, epoch 32\n",
      "Step: 0, Loss: 0.2144133299589157\n",
      "Step: 100, Loss: 0.25546079874038696\n",
      "Step: 200, Loss: 0.278865247964859\n",
      "Step: 300, Loss: 0.34178438782691956\n",
      "Step: 400, Loss: 0.40096527338027954\n",
      "Step: 500, Loss: 0.3401295840740204\n",
      "Step: 600, Loss: 0.31134602427482605\n",
      "New Data, epoch 33\n",
      "Step: 0, Loss: 0.22376801073551178\n",
      "Step: 100, Loss: 0.3092517554759979\n",
      "Step: 200, Loss: 0.27899307012557983\n",
      "Step: 300, Loss: 0.3086579740047455\n",
      "Step: 400, Loss: 0.4254817068576813\n",
      "Step: 500, Loss: 0.3116491138935089\n",
      "Step: 600, Loss: 0.3374931514263153\n",
      "New Data, epoch 34\n",
      "Step: 0, Loss: 0.2478022575378418\n",
      "Step: 100, Loss: 0.3105837106704712\n",
      "Step: 200, Loss: 0.39924493432044983\n",
      "Step: 300, Loss: 0.3093266189098358\n",
      "Step: 400, Loss: 0.3392638862133026\n",
      "Step: 500, Loss: 0.24817591905593872\n",
      "Step: 600, Loss: 0.4271030128002167\n",
      "New Data, epoch 35\n",
      "Step: 0, Loss: 0.22732791304588318\n",
      "Step: 100, Loss: 0.2800004184246063\n",
      "Step: 200, Loss: 0.27830684185028076\n",
      "Step: 300, Loss: 0.24980854988098145\n",
      "Step: 400, Loss: 0.3679450750350952\n",
      "Step: 500, Loss: 0.28831297159194946\n",
      "Step: 600, Loss: 0.40197691321372986\n",
      "New Data, epoch 36\n",
      "Step: 0, Loss: 0.24000780284404755\n",
      "Step: 100, Loss: 0.37231504917144775\n",
      "Step: 200, Loss: 0.308254212141037\n",
      "Step: 300, Loss: 0.34449636936187744\n",
      "Step: 400, Loss: 0.3727439343929291\n",
      "Step: 500, Loss: 0.34199097752571106\n",
      "Step: 600, Loss: 0.27963826060295105\n",
      "New Data, epoch 37\n",
      "Step: 0, Loss: 0.20799055695533752\n",
      "Step: 100, Loss: 0.335875540971756\n",
      "Step: 200, Loss: 0.3407529890537262\n",
      "Step: 300, Loss: 0.37582752108573914\n",
      "Step: 400, Loss: 0.3090839982032776\n",
      "Step: 500, Loss: 0.28023985028266907\n",
      "Step: 600, Loss: 0.3773379325866699\n",
      "New Data, epoch 38\n",
      "Step: 0, Loss: 0.2332596331834793\n",
      "Step: 100, Loss: 0.3072260618209839\n",
      "Step: 200, Loss: 0.3096221089363098\n",
      "Step: 300, Loss: 0.30940914154052734\n",
      "Step: 400, Loss: 0.31098443269729614\n",
      "Step: 500, Loss: 0.310477614402771\n",
      "Step: 600, Loss: 0.3100087344646454\n",
      "New Data, epoch 39\n",
      "Step: 0, Loss: 0.2098018229007721\n",
      "Step: 100, Loss: 0.28084662556648254\n",
      "Step: 200, Loss: 0.31238600611686707\n",
      "Step: 300, Loss: 0.3132011890411377\n",
      "Step: 400, Loss: 0.3080127537250519\n",
      "Step: 500, Loss: 0.37595635652542114\n",
      "Step: 600, Loss: 0.3396475315093994\n",
      "New Data, epoch 40\n",
      "Step: 0, Loss: 0.22049853205680847\n",
      "Step: 100, Loss: 0.22275342047214508\n",
      "Step: 200, Loss: 0.4009469747543335\n",
      "Step: 300, Loss: 0.31191569566726685\n",
      "Step: 400, Loss: 0.370627760887146\n",
      "Step: 500, Loss: 0.2804933488368988\n",
      "Step: 600, Loss: 0.3126351833343506\n",
      "New Data, epoch 41\n",
      "Step: 0, Loss: 0.23821236193180084\n",
      "Step: 100, Loss: 0.31523042917251587\n",
      "Step: 200, Loss: 0.24943041801452637\n",
      "Step: 300, Loss: 0.28014105558395386\n",
      "Step: 400, Loss: 0.3073204457759857\n",
      "Step: 500, Loss: 0.36931514739990234\n",
      "Step: 600, Loss: 0.31056830286979675\n",
      "New Data, epoch 42\n",
      "Step: 0, Loss: 0.2549643814563751\n",
      "Step: 100, Loss: 0.3087144196033478\n",
      "Step: 200, Loss: 0.31242635846138\n",
      "Step: 300, Loss: 0.2813359200954437\n",
      "Step: 400, Loss: 0.4315497875213623\n",
      "Step: 500, Loss: 0.3077831566333771\n",
      "Step: 600, Loss: 0.362911194562912\n",
      "New Data, epoch 43\n",
      "Step: 0, Loss: 0.24222078919410706\n",
      "Step: 100, Loss: 0.22300845384597778\n",
      "Step: 200, Loss: 0.36651700735092163\n",
      "Step: 300, Loss: 0.2852124571800232\n",
      "Step: 400, Loss: 0.34472596645355225\n",
      "Step: 500, Loss: 0.28308001160621643\n",
      "Step: 600, Loss: 0.39923638105392456\n",
      "New Data, epoch 44\n",
      "Step: 0, Loss: 0.23493970930576324\n",
      "Step: 100, Loss: 0.3081234395503998\n",
      "Step: 200, Loss: 0.28464892506599426\n",
      "Step: 300, Loss: 0.33734703063964844\n",
      "Step: 400, Loss: 0.3431229293346405\n",
      "Step: 500, Loss: 0.3406226634979248\n",
      "Step: 600, Loss: 0.368850976228714\n",
      "New Data, epoch 45\n",
      "Step: 0, Loss: 0.24025894701480865\n",
      "Step: 100, Loss: 0.3107108473777771\n",
      "Step: 200, Loss: 0.4015394449234009\n",
      "Step: 300, Loss: 0.2823949456214905\n",
      "Step: 400, Loss: 0.25577831268310547\n",
      "Step: 500, Loss: 0.39657944440841675\n",
      "Step: 600, Loss: 0.3771531283855438\n",
      "New Data, epoch 46\n",
      "Step: 0, Loss: 0.25845223665237427\n",
      "Step: 100, Loss: 0.27785685658454895\n",
      "Step: 200, Loss: 0.3381352722644806\n",
      "Step: 300, Loss: 0.18607378005981445\n",
      "Step: 400, Loss: 0.3393375873565674\n",
      "Step: 500, Loss: 0.3104866147041321\n",
      "Step: 600, Loss: 0.33954915404319763\n",
      "New Data, epoch 47\n",
      "Step: 0, Loss: 0.2020684778690338\n",
      "Step: 100, Loss: 0.22046233713626862\n",
      "Step: 200, Loss: 0.369376003742218\n",
      "Step: 300, Loss: 0.3399886190891266\n",
      "Step: 400, Loss: 0.28325480222702026\n",
      "Step: 500, Loss: 0.24737797677516937\n",
      "Step: 600, Loss: 0.31305140256881714\n",
      "New Data, epoch 48\n",
      "Step: 0, Loss: 0.20996107161045074\n",
      "Step: 100, Loss: 0.3092530071735382\n",
      "Step: 200, Loss: 0.2514837980270386\n",
      "Step: 300, Loss: 0.3137490451335907\n",
      "Step: 400, Loss: 0.19671036303043365\n",
      "Step: 500, Loss: 0.3139769732952118\n",
      "Step: 600, Loss: 0.3665107190608978\n",
      "New Data, epoch 49\n",
      "Step: 0, Loss: 0.23622049391269684\n",
      "Step: 100, Loss: 0.3124600946903229\n",
      "Step: 200, Loss: 0.2563888430595398\n",
      "Step: 300, Loss: 0.3715168237686157\n",
      "Step: 400, Loss: 0.21807530522346497\n",
      "Step: 500, Loss: 0.25222495198249817\n",
      "Step: 600, Loss: 0.31211352348327637\n",
      "New Data, epoch 50\n",
      "Step: 0, Loss: 0.25110089778900146\n",
      "Step: 100, Loss: 0.3375416398048401\n",
      "Step: 200, Loss: 0.36845067143440247\n",
      "Step: 300, Loss: 0.33807751536369324\n",
      "Step: 400, Loss: 0.3411082625389099\n",
      "Step: 500, Loss: 0.19286829233169556\n",
      "Step: 600, Loss: 0.3169454336166382\n",
      "New Data, epoch 51\n",
      "Step: 0, Loss: 0.22980128228664398\n",
      "Step: 100, Loss: 0.3111397624015808\n",
      "Step: 200, Loss: 0.3095565438270569\n",
      "Step: 300, Loss: 0.2731209993362427\n",
      "Step: 400, Loss: 0.24944078922271729\n",
      "Step: 500, Loss: 0.4006481170654297\n",
      "Step: 600, Loss: 0.33901602029800415\n",
      "New Data, epoch 52\n",
      "Step: 0, Loss: 0.23721684515476227\n",
      "Step: 100, Loss: 0.33847323060035706\n",
      "Step: 200, Loss: 0.3676426410675049\n",
      "Step: 300, Loss: 0.42926105856895447\n",
      "Step: 400, Loss: 0.39535781741142273\n",
      "Step: 500, Loss: 0.343255877494812\n",
      "Step: 600, Loss: 0.31345057487487793\n",
      "New Data, epoch 53\n",
      "Step: 0, Loss: 0.2079429030418396\n",
      "Step: 100, Loss: 0.369833767414093\n",
      "Step: 200, Loss: 0.22438398003578186\n",
      "Step: 300, Loss: 0.3143428564071655\n",
      "Step: 400, Loss: 0.3420328199863434\n",
      "Step: 500, Loss: 0.341569721698761\n",
      "Step: 600, Loss: 0.3133573830127716\n",
      "New Data, epoch 54\n",
      "Step: 0, Loss: 0.22958464920520782\n",
      "Step: 100, Loss: 0.27655893564224243\n",
      "Step: 200, Loss: 0.37285301089286804\n",
      "Step: 300, Loss: 0.3711751401424408\n",
      "Step: 400, Loss: 0.33834558725357056\n",
      "Step: 500, Loss: 0.2521810233592987\n",
      "Step: 600, Loss: 0.3099188804626465\n",
      "New Data, epoch 55\n",
      "Step: 0, Loss: 0.26737648248672485\n",
      "Step: 100, Loss: 0.2167970836162567\n",
      "Step: 200, Loss: 0.18963280320167542\n",
      "Step: 300, Loss: 0.25422587990760803\n",
      "Step: 400, Loss: 0.3983924686908722\n",
      "Step: 500, Loss: 0.3411552309989929\n",
      "Step: 600, Loss: 0.30968695878982544\n",
      "New Data, epoch 56\n",
      "Step: 0, Loss: 0.23885676264762878\n",
      "Step: 100, Loss: 0.2542463541030884\n",
      "Step: 200, Loss: 0.3678949773311615\n",
      "Step: 300, Loss: 0.40100279450416565\n",
      "Step: 400, Loss: 0.3126206696033478\n",
      "Step: 500, Loss: 0.2548889219760895\n",
      "Step: 600, Loss: 0.31030112504959106\n",
      "New Data, epoch 57\n",
      "Step: 0, Loss: 0.23916247487068176\n",
      "Step: 100, Loss: 0.25538012385368347\n",
      "Step: 200, Loss: 0.2808619439601898\n",
      "Step: 300, Loss: 0.22474978864192963\n",
      "Step: 400, Loss: 0.37335237860679626\n",
      "Step: 500, Loss: 0.3398582637310028\n",
      "Step: 600, Loss: 0.28010255098342896\n",
      "New Data, epoch 58\n",
      "Step: 0, Loss: 0.2781945765018463\n",
      "Step: 100, Loss: 0.30917879939079285\n",
      "Step: 200, Loss: 0.22190284729003906\n",
      "Step: 300, Loss: 0.366879403591156\n",
      "Step: 400, Loss: 0.31360167264938354\n",
      "Step: 500, Loss: 0.2752726078033447\n",
      "Step: 600, Loss: 0.2809891998767853\n",
      "New Data, epoch 59\n",
      "Step: 0, Loss: 0.2609010338783264\n",
      "Step: 100, Loss: 0.23603154718875885\n",
      "Step: 200, Loss: 0.3385109007358551\n",
      "Step: 300, Loss: 0.3139853775501251\n",
      "Step: 400, Loss: 0.3995169997215271\n",
      "Step: 500, Loss: 0.278058260679245\n",
      "Step: 600, Loss: 0.25655511021614075\n",
      "New Data, epoch 60\n",
      "Step: 0, Loss: 0.25438445806503296\n",
      "Step: 100, Loss: 0.37048566341400146\n",
      "Step: 200, Loss: 0.30896633863449097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 300, Loss: 0.31209346652030945\n",
      "Step: 400, Loss: 0.36920174956321716\n",
      "Step: 500, Loss: 0.36722829937934875\n",
      "Step: 600, Loss: 0.36998313665390015\n",
      "New Data, epoch 61\n",
      "Step: 0, Loss: 0.2201496958732605\n",
      "Step: 100, Loss: 0.3441466987133026\n",
      "Step: 200, Loss: 0.3106577694416046\n",
      "Step: 300, Loss: 0.2810341715812683\n",
      "Step: 400, Loss: 0.3132396936416626\n",
      "Step: 500, Loss: 0.3700883686542511\n",
      "Step: 600, Loss: 0.4271877408027649\n",
      "New Data, epoch 62\n",
      "Step: 0, Loss: 0.24713531136512756\n",
      "Step: 100, Loss: 0.2544262111186981\n",
      "Step: 200, Loss: 0.30600666999816895\n",
      "Step: 300, Loss: 0.33866462111473083\n",
      "Step: 400, Loss: 0.28480803966522217\n",
      "Step: 500, Loss: 0.36976316571235657\n",
      "Step: 600, Loss: 0.4272898733615875\n",
      "New Data, epoch 63\n",
      "Step: 0, Loss: 0.21703974902629852\n",
      "Step: 100, Loss: 0.39829888939857483\n",
      "Step: 200, Loss: 0.3962586522102356\n",
      "Step: 300, Loss: 0.28082752227783203\n",
      "Step: 400, Loss: 0.39671066403388977\n",
      "Step: 500, Loss: 0.307517409324646\n",
      "Step: 600, Loss: 0.33296698331832886\n",
      "New Data, epoch 64\n",
      "Step: 0, Loss: 0.20844793319702148\n",
      "Step: 100, Loss: 0.28201135993003845\n",
      "Step: 200, Loss: 0.36806389689445496\n",
      "Step: 300, Loss: 0.2793387472629547\n",
      "Step: 400, Loss: 0.3401225209236145\n",
      "Step: 500, Loss: 0.33978816866874695\n",
      "Step: 600, Loss: 0.3403778672218323\n",
      "New Data, epoch 65\n",
      "Step: 0, Loss: 0.2364118993282318\n",
      "Step: 100, Loss: 0.33939293026924133\n",
      "Step: 200, Loss: 0.4024782180786133\n",
      "Step: 300, Loss: 0.15997056663036346\n",
      "Step: 400, Loss: 0.3117811977863312\n",
      "Step: 500, Loss: 0.429340660572052\n",
      "Step: 600, Loss: 0.2797960937023163\n",
      "New Data, epoch 66\n",
      "Step: 0, Loss: 0.2782096266746521\n",
      "Step: 100, Loss: 0.24908238649368286\n",
      "Step: 200, Loss: 0.2490958422422409\n",
      "Step: 300, Loss: 0.3403952419757843\n",
      "Step: 400, Loss: 0.33399146795272827\n",
      "Step: 500, Loss: 0.19463349878787994\n",
      "Step: 600, Loss: 0.25276029109954834\n",
      "New Data, epoch 67\n",
      "Step: 0, Loss: 0.24832814931869507\n",
      "Step: 100, Loss: 0.36815765500068665\n",
      "Step: 200, Loss: 0.3088553249835968\n",
      "Step: 300, Loss: 0.3148696720600128\n",
      "Step: 400, Loss: 0.30976492166519165\n",
      "Step: 500, Loss: 0.24755164980888367\n",
      "Step: 600, Loss: 0.37326714396476746\n",
      "New Data, epoch 68\n",
      "Step: 0, Loss: 0.22672556340694427\n",
      "Step: 100, Loss: 0.3082028925418854\n",
      "Step: 200, Loss: 0.28149840235710144\n",
      "Step: 300, Loss: 0.39970266819000244\n",
      "Step: 400, Loss: 0.34014788269996643\n",
      "Step: 500, Loss: 0.2795369625091553\n",
      "Step: 600, Loss: 0.3408476412296295\n",
      "New Data, epoch 69\n",
      "Step: 0, Loss: 0.2585967183113098\n",
      "Step: 100, Loss: 0.2790026068687439\n",
      "Step: 200, Loss: 0.2516915798187256\n",
      "Step: 300, Loss: 0.24980343878269196\n",
      "Step: 400, Loss: 0.3947617709636688\n",
      "Step: 500, Loss: 0.21448707580566406\n",
      "Step: 600, Loss: 0.30895406007766724\n",
      "New Data, epoch 70\n",
      "Step: 0, Loss: 0.21933941543102264\n",
      "Step: 100, Loss: 0.4297647178173065\n",
      "Step: 200, Loss: 0.3139292597770691\n",
      "Step: 300, Loss: 0.4000985324382782\n",
      "Step: 400, Loss: 0.3981418013572693\n",
      "Step: 500, Loss: 0.31122708320617676\n",
      "Step: 600, Loss: 0.34602561593055725\n",
      "New Data, epoch 71\n",
      "Step: 0, Loss: 0.23439957201480865\n",
      "Step: 100, Loss: 0.3714814782142639\n",
      "Step: 200, Loss: 0.338614284992218\n",
      "Step: 300, Loss: 0.33956047892570496\n",
      "Step: 400, Loss: 0.2783498167991638\n",
      "Step: 500, Loss: 0.37234392762184143\n",
      "Step: 600, Loss: 0.3412021994590759\n",
      "New Data, epoch 72\n",
      "Step: 0, Loss: 0.21672140061855316\n",
      "Step: 100, Loss: 0.3387019634246826\n",
      "Step: 200, Loss: 0.3122585415840149\n",
      "Step: 300, Loss: 0.33794915676116943\n",
      "Step: 400, Loss: 0.37106698751449585\n",
      "Step: 500, Loss: 0.34303465485572815\n",
      "Step: 600, Loss: 0.3675440847873688\n",
      "New Data, epoch 73\n",
      "Step: 0, Loss: 0.24573981761932373\n",
      "Step: 100, Loss: 0.25395774841308594\n",
      "Step: 200, Loss: 0.36682483553886414\n",
      "Step: 300, Loss: 0.3403981924057007\n",
      "Step: 400, Loss: 0.3677821755409241\n",
      "Step: 500, Loss: 0.3675645589828491\n",
      "Step: 600, Loss: 0.39781635999679565\n",
      "New Data, epoch 74\n",
      "Step: 0, Loss: 0.23206458985805511\n",
      "Step: 100, Loss: 0.3703485131263733\n",
      "Step: 200, Loss: 0.24964851140975952\n",
      "Step: 300, Loss: 0.34008553624153137\n",
      "Step: 400, Loss: 0.3099052608013153\n",
      "Step: 500, Loss: 0.310011625289917\n",
      "Step: 600, Loss: 0.34320515394210815\n",
      "New Data, epoch 75\n",
      "Step: 0, Loss: 0.2097276747226715\n",
      "Step: 100, Loss: 0.3115605413913727\n",
      "Step: 200, Loss: 0.225521981716156\n",
      "Step: 300, Loss: 0.39728179574012756\n",
      "Step: 400, Loss: 0.2190784513950348\n",
      "Step: 500, Loss: 0.2825557291507721\n",
      "Step: 600, Loss: 0.3694079518318176\n",
      "New Data, epoch 76\n",
      "Step: 0, Loss: 0.23565205931663513\n",
      "Step: 100, Loss: 0.2544671297073364\n",
      "Step: 200, Loss: 0.28211331367492676\n",
      "Step: 300, Loss: 0.36738651990890503\n",
      "Step: 400, Loss: 0.3077828586101532\n",
      "Step: 500, Loss: 0.37423089146614075\n",
      "Step: 600, Loss: 0.3100587725639343\n",
      "New Data, epoch 77\n",
      "Step: 0, Loss: 0.24235951900482178\n",
      "Step: 100, Loss: 0.3970358669757843\n",
      "Step: 200, Loss: 0.40074193477630615\n",
      "Step: 300, Loss: 0.27928102016448975\n",
      "Step: 400, Loss: 0.33701300621032715\n",
      "Step: 500, Loss: 0.33916956186294556\n",
      "Step: 600, Loss: 0.217433899641037\n",
      "New Data, epoch 78\n",
      "Step: 0, Loss: 0.22841952741146088\n",
      "Step: 100, Loss: 0.3686768412590027\n",
      "Step: 200, Loss: 0.25230672955513\n",
      "Step: 300, Loss: 0.28291624784469604\n",
      "Step: 400, Loss: 0.4020901322364807\n",
      "Step: 500, Loss: 0.3132939636707306\n",
      "Step: 600, Loss: 0.42765507102012634\n",
      "New Data, epoch 79\n",
      "Step: 0, Loss: 0.233286052942276\n",
      "Step: 100, Loss: 0.3083949387073517\n",
      "Step: 200, Loss: 0.3396121859550476\n",
      "Step: 300, Loss: 0.31124353408813477\n",
      "Step: 400, Loss: 0.3376412093639374\n",
      "Step: 500, Loss: 0.3685914874076843\n",
      "Step: 600, Loss: 0.28210803866386414\n",
      "New Data, epoch 80\n",
      "Step: 0, Loss: 0.20020359754562378\n",
      "Step: 100, Loss: 0.4299667775630951\n",
      "Step: 200, Loss: 0.2793867290019989\n",
      "Step: 300, Loss: 0.31039923429489136\n",
      "Step: 400, Loss: 0.37196019291877747\n",
      "Step: 500, Loss: 0.3388931453227997\n",
      "Step: 600, Loss: 0.369575560092926\n",
      "New Data, epoch 81\n",
      "Step: 0, Loss: 0.22621604800224304\n",
      "Step: 100, Loss: 0.25155016779899597\n",
      "Step: 200, Loss: 0.34110936522483826\n",
      "Step: 300, Loss: 0.311123788356781\n",
      "Step: 400, Loss: 0.39715492725372314\n",
      "Step: 500, Loss: 0.3092201054096222\n",
      "Step: 600, Loss: 0.2807151675224304\n",
      "New Data, epoch 82\n",
      "Step: 0, Loss: 0.21336403489112854\n",
      "Step: 100, Loss: 0.31249117851257324\n",
      "Step: 200, Loss: 0.30912530422210693\n",
      "Step: 300, Loss: 0.25839224457740784\n",
      "Step: 400, Loss: 0.25132128596305847\n",
      "Step: 500, Loss: 0.3387761414051056\n",
      "Step: 600, Loss: 0.2840859889984131\n",
      "New Data, epoch 83\n",
      "Step: 0, Loss: 0.2463505119085312\n",
      "Step: 100, Loss: 0.3387683033943176\n",
      "Step: 200, Loss: 0.22156895697116852\n",
      "Step: 300, Loss: 0.3704036474227905\n",
      "Step: 400, Loss: 0.33642083406448364\n",
      "Step: 500, Loss: 0.3402272164821625\n",
      "Step: 600, Loss: 0.3130037486553192\n",
      "New Data, epoch 84\n",
      "Step: 0, Loss: 0.22444859147071838\n",
      "Step: 100, Loss: 0.3393760025501251\n",
      "Step: 200, Loss: 0.25490203499794006\n",
      "Step: 300, Loss: 0.3399532735347748\n",
      "Step: 400, Loss: 0.311662495136261\n",
      "Step: 500, Loss: 0.2195206731557846\n",
      "Step: 600, Loss: 0.2534095346927643\n",
      "New Data, epoch 85\n",
      "Step: 0, Loss: 0.24996055662631989\n",
      "Step: 100, Loss: 0.27929985523223877\n",
      "Step: 200, Loss: 0.31173649430274963\n",
      "Step: 300, Loss: 0.33734041452407837\n",
      "Step: 400, Loss: 0.3711321949958801\n",
      "Step: 500, Loss: 0.4024169445037842\n",
      "Step: 600, Loss: 0.36923453211784363\n",
      "New Data, epoch 86\n",
      "Step: 0, Loss: 0.24429461359977722\n",
      "Step: 100, Loss: 0.3980861008167267\n",
      "Step: 200, Loss: 0.2801133096218109\n",
      "Step: 300, Loss: 0.37073132395744324\n",
      "Step: 400, Loss: 0.3715822696685791\n",
      "Step: 500, Loss: 0.48246946930885315\n",
      "Step: 600, Loss: 0.3386748135089874\n",
      "New Data, epoch 87\n",
      "Step: 0, Loss: 0.23575632274150848\n",
      "Step: 100, Loss: 0.28231605887413025\n",
      "Step: 200, Loss: 0.28125375509262085\n",
      "Step: 300, Loss: 0.338037371635437\n",
      "Step: 400, Loss: 0.27917176485061646\n",
      "Step: 500, Loss: 0.307345986366272\n",
      "Step: 600, Loss: 0.37211087346076965\n",
      "New Data, epoch 88\n",
      "Step: 0, Loss: 0.23780088126659393\n",
      "Step: 100, Loss: 0.4000721275806427\n",
      "Step: 200, Loss: 0.30937841534614563\n",
      "Step: 300, Loss: 0.3462555706501007\n",
      "Step: 400, Loss: 0.36526191234588623\n",
      "Step: 500, Loss: 0.28427329659461975\n",
      "Step: 600, Loss: 0.28416574001312256\n",
      "New Data, epoch 89\n",
      "Step: 0, Loss: 0.20559965074062347\n",
      "Step: 100, Loss: 0.27862298488616943\n",
      "Step: 200, Loss: 0.34403204917907715\n",
      "Step: 300, Loss: 0.3964865803718567\n",
      "Step: 400, Loss: 0.4006059467792511\n",
      "Step: 500, Loss: 0.34506601095199585\n",
      "Step: 600, Loss: 0.25093573331832886\n",
      "New Data, epoch 90\n",
      "Step: 0, Loss: 0.2526371479034424\n",
      "Step: 100, Loss: 0.27947601675987244\n",
      "Step: 200, Loss: 0.3114916682243347\n",
      "Step: 300, Loss: 0.24740730226039886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 400, Loss: 0.22245854139328003\n",
      "Step: 500, Loss: 0.3086817264556885\n",
      "Step: 600, Loss: 0.36758193373680115\n",
      "New Data, epoch 91\n",
      "Step: 0, Loss: 0.2219458520412445\n",
      "Step: 100, Loss: 0.37207937240600586\n",
      "Step: 200, Loss: 0.31147122383117676\n",
      "Step: 300, Loss: 0.30882781744003296\n",
      "Step: 400, Loss: 0.34865260124206543\n",
      "Step: 500, Loss: 0.3373330533504486\n",
      "Step: 600, Loss: 0.36798369884490967\n",
      "New Data, epoch 92\n",
      "Step: 0, Loss: 0.23088236153125763\n",
      "Step: 100, Loss: 0.30736827850341797\n",
      "Step: 200, Loss: 0.22391855716705322\n",
      "Step: 300, Loss: 0.36913013458251953\n",
      "Step: 400, Loss: 0.3410896360874176\n",
      "Step: 500, Loss: 0.3399641811847687\n",
      "Step: 600, Loss: 0.3115423619747162\n",
      "New Data, epoch 93\n",
      "Step: 0, Loss: 0.22599206864833832\n",
      "Step: 100, Loss: 0.21484842896461487\n",
      "Step: 200, Loss: 0.398555725812912\n",
      "Step: 300, Loss: 0.4558994174003601\n",
      "Step: 400, Loss: 0.34316694736480713\n",
      "Step: 500, Loss: 0.3086968660354614\n",
      "Step: 600, Loss: 0.36789068579673767\n",
      "New Data, epoch 94\n",
      "Step: 0, Loss: 0.23435740172863007\n",
      "Step: 100, Loss: 0.2788834273815155\n",
      "Step: 200, Loss: 0.3975929319858551\n",
      "Step: 300, Loss: 0.36870911717414856\n",
      "Step: 400, Loss: 0.2540357708930969\n",
      "Step: 500, Loss: 0.2787235379219055\n",
      "Step: 600, Loss: 0.3717491626739502\n",
      "New Data, epoch 95\n",
      "Step: 0, Loss: 0.24119412899017334\n",
      "Step: 100, Loss: 0.36983272433280945\n",
      "Step: 200, Loss: 0.27913975715637207\n",
      "Step: 300, Loss: 0.3701639175415039\n",
      "Step: 400, Loss: 0.24816569685935974\n",
      "Step: 500, Loss: 0.3303450644016266\n",
      "Step: 600, Loss: 0.3686179518699646\n",
      "New Data, epoch 96\n",
      "Step: 0, Loss: 0.23121657967567444\n",
      "Step: 100, Loss: 0.22084951400756836\n",
      "Step: 200, Loss: 0.30629289150238037\n",
      "Step: 300, Loss: 0.3354257047176361\n",
      "Step: 400, Loss: 0.31367602944374084\n",
      "Step: 500, Loss: 0.24832196533679962\n",
      "Step: 600, Loss: 0.33960825204849243\n",
      "New Data, epoch 97\n",
      "Step: 0, Loss: 0.2754034996032715\n",
      "Step: 100, Loss: 0.2810555398464203\n",
      "Step: 200, Loss: 0.3418194353580475\n",
      "Step: 300, Loss: 0.3086964786052704\n",
      "Step: 400, Loss: 0.4003426432609558\n",
      "Step: 500, Loss: 0.3946181535720825\n",
      "Step: 600, Loss: 0.36776402592658997\n",
      "New Data, epoch 98\n",
      "Step: 0, Loss: 0.19981352984905243\n",
      "Step: 100, Loss: 0.3652016222476959\n",
      "Step: 200, Loss: 0.3383170962333679\n",
      "Step: 300, Loss: 0.2193959802389145\n",
      "Step: 400, Loss: 0.31417766213417053\n",
      "Step: 500, Loss: 0.3064081072807312\n",
      "Step: 600, Loss: 0.3696898818016052\n",
      "New Data, epoch 99\n",
      "Step: 0, Loss: 0.23969992995262146\n",
      "Step: 100, Loss: 0.30905038118362427\n",
      "Step: 200, Loss: 0.3158181309700012\n",
      "Step: 300, Loss: 0.39144548773765564\n",
      "Step: 400, Loss: 0.31462380290031433\n",
      "Step: 500, Loss: 0.3124793767929077\n",
      "Step: 600, Loss: 0.19369354844093323\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3X+YFPWV7/H3YfgliMIIKBmYAIIogj9wAONyid4YQTTL\nejUJ7N4ogiEazJM8N2bDalRWd7Oa3E2yRiNLDDfR3ejGJGvYFcZoIuuPRHHGRRASBEUFgiCi/BSY\nGc79o2uGnp7unu6e6l/Vn9fzzEN31beqztRpzlTXt6q+5u6IiEh0dSt2ACIikl8q9CIiEadCLyIS\ncSr0IiIRp0IvIhJxKvQiIhGnQl+hzGyYmT1tZuvNbJ2ZfTlJGzOze8xsk5mtMbMJxYhVMqe8SjLd\nix2AFE0z8FV3f9nM+gGNZvaku6+Pa3MpMDr4mQzcH/wrpUt5lQ50RF+h3H27u78cvN4H/AGoSWg2\nE3jQY14A+pvZkAKHKllQXiWZoh3RDxw40IcPH16szUucxsbG3cAB4MWEWTXAlrj3W4Np2+Mbmdl8\nYD5A3759zzv99NPzF6xkrKt5BeW2FDU2Nu5y90HZLFO0Qj98+HAaGhqKtXkJ7N+/n379+vUGPu/u\ne3NZh7svAZYA1NXVufJafGHkFZTbUmRmb2W7jE7dVLCmpiauvPJKgN3u/sskTbYBw+LeDw2mSQlT\nXiWRCn2FcnfmzZvHGWecAbAjRbNlwNXBVRrnA3vcvcPXeykdyqskUxJX3RxpPspp31jBrZePZd6U\nEcUOpyI8//zzPPTQQ4wfPx5grJmtBm4GagHcfTGwHJgBbAIOAtcWKVzJkPIqyZREof/vt98H4M7/\nXK9CXyBTpkyh9RHVZrbe3esS23iswYJCxya5U14lmZI4dXOwqaXYIYiIRFZJFPphA44rdggiIpFV\nEoVeRETyp0QKvRU7ABGRyCqRQq9xa0VE8qUkCv1R1XkRkbwpiUI/uF+vYocgIhJZJVHo+/fpycDj\nVexFRPKhJAo9wJ+f/RH69SqJ+7dERCKlZAo9qEtWRCQfSqbQm66wFBHJi4wKvZlNN7MNwRiTC1O0\nudDMVgfjVP5XLsG0PqNDRETC0+lJcTOrAu4DPklsJJqXzGxZ/BiUZtYf+AEw3d3fNrPB2QZi6NSN\niEg+ZHJEPwnY5O5vuPsR4BFiY07G+0vgl+7+NoC778w2EJ26ERHJj0wKfarxJeOdBgwws5Vm1mhm\nVydbkZnNN7MGM2t49913O8zXmRsRkfCF1RnbHTgPuAyYBtxqZqclNnL3Je5e5+51gwa1H9vWdEgv\nIpIXmRT6TMaX3Ao84e4H3H0X8AxwdrbBuM7SF8zcuXMZPHgw48aNSzo/6FzfE3Swrzaz2wocouSo\nNbfAmcnmK7eVJ5NC/xIw2sxGmFlPYBaxMSfj/QqYYmbdzawPMBn4QzaBGDp1U0hz5syhvr6+s2bP\nuvs5wc8dhYhLuk65lUSdFnp3bwZuBJ4gVrx/5u7rzOx6M7s+aPMHoB5YA6wCHnD3V7OKRGduCmrq\n1KlUV1cXOwzJA+VWEmX0zAF3X05sQOH4aYsT3n8b+HZXgtEBfcm5wMzWEDtVd5O7r0vWyMzmA/MB\namtrCxiedIFyW0FK585YHdKXmpeBWnc/C/g+8Fiqhuk62aUkKbcVpmQKPaBD+hLi7nvdfX/wejnQ\nw8wGFjksCYFyW3lKptCblf9VN80tR9m4Y1+xwwiFmZ1iwTWvZjaJ2GflveJGJWFQbitPyTwXOJMT\nN69u28MHB5uYMro0Dz7uWvFHHnhuM8987SJqT+pT7HDSmj17NitXrmTXrl0AZ5nZPKAHtPW/XAXc\nYGbNwIfALNfDiMpCa26BXma2Fbgd5bailUyhh9SXVza3HKV7VTcu//5zALx512Vd2s5rO/ZRW92H\n3j2qurSeRC+99T4A7x04nHOhf2XLBzQfdc776IAwQ+vg4YcfbnttZmvc/Ufx8939XuDevAYhedGa\nWzN72d3rEucrt5WnZE7dtLjTfNQ51NQCwGnfWMGnF/+OP76zl1G3rOCJde+kXHb3gSO8/d7BjLaz\n/3Azl3z3Gb7yyOqcYz3U1ML7B45k3P5Xq7elPaXT1HKUfYeaAJh53/Ncef/vaMlgIF1354/v7M0q\nFhGpPCVT6H/2UuxxOvevfB2AI81HeenN93lu4y4Afr1uR7v2TS1H+fvH1/OT373JhDufZOq3n065\n7n2Hmhi+8HGGL3ycw8Efkvp177D1/YO8t/9wRvH9+PnNvLnrAB8cPMLpt9Zz7p1P8rVHX2mbf+4d\nv+aVLR8kXfbLj6zmk999hnf2HGqb9oOVm1i7dQ8AX3iokfGLfk3jW7vb5n/+wYZOY/pZwxamf+9Z\nzr3zyZRt9nzYxJ8++LDTdYlIdJVMoT94JFaADxxubjf9teBI+Bcvb22btudgE4+v2c4Pn93M7cuS\nXv7bzv97/s221/HHyVPufprz/u6pTpc/1NTCov9Yz6f/+ffs3HfsD8OjjVs5Ghx5v3+wqW36FT/4\nHYebWzqs5/x/+A3r/7QXgG/Vb+BT98ZORf32j7GHfV55/+/b2rZOS+VfXniLr/9ibaexX/Ld/+KC\nu37baTsRia6SKfStzzR74LnNbHgn/ZUrZ9/xa+5a8ccCRBXT2nfQenol3vsHk5822fthc9Lpb713\nIJSYvvFY+xuPU+2zHXsz+8YiItFVMoU+3n+//X7b61RHtu/sPZR0OnQsyPFX9PzwmTe6FFvY0v1R\n2xn8jk0tRzlwuBl3Z/OuA0lPEd3wL43t3u871MSHR9p/q3htxz52pNlvIhJNJXPVTfydsbvjjpJ3\n7c+8o3HTzv1c/J3YKIZfuXg0X7n4NL731Gv89MW329r8c5JC/7kfvchHT+rDZeM/wjceW8v3Pnsu\npw/px+hbVnDJ2JO54NSTADjUdJRt77c/371x5/6kp3+OtBzlxp++TI+q9n9LX9uxn3Nqj61j2vee\nSfn7TPrmbziuRxUfBv0KX5s2hm8/sSFp2zd2HeC7T77GkBN782FTC3/7H+vbzf/cj17k2aC/o6tX\nLYlIebFiXT5bV1fnDQ3HOhzPuLW+raDl6tPnDeXRxmPn8qedeTJPJHTiplPVzdqudnlw7iSuXrqq\nQ5sTendn76Hkp2Xy7exh/VN2+GYjsdCbWWOyy/BykZhXKZ4w8wrKbanIJa8lc+omjHFH4os8kFWR\nB9pd0pisyIMepSwi5adkCr2IiORHyRT6g0e6dtqmUPYdLs5pGyCU0zYiUnlKptCLiEh+qNCLiESc\nCr2ISMSp0IuIRJwKvYhIxKnQi4hEXEaF3symm9kGM9tkZgvTtJtoZs1mdlV4IUo+zJ07l8GDBzNu\n3Lik8y3mniDna8xsQoFDlBy15hY4M9l85bbydFrozawKuA+4FBgLzDazsSna3Q38OuwgJXxz5syh\nvr4+XZNLgdHBz3zg/kLEJV2n3EqiTI7oJwGb3P0Ndz8CPALMTNLuS8AvgPQPUpeSMHXqVKqrq9M1\nmQk86DEvAP3NbEhhopOuUG4lUSZPr6wBtsS93wpMjm9gZjXAFcBFwMRUKzKz+cSOIKitrc02Vims\nZHmvAbYnNozPK9S2e25RqmcD5fJso3TPGUq1vlyWyVUuv2suz04KYX2h5Dbb7RdyP5TydnL5rHZV\nWJ2x3wO+7u5H0zVy9yXuXufudYMGDQpp01Js8XkF5TVKlNtoyOSIfhswLO790GBavDrgEYv9qRoI\nzDCzZnd/LJQopRgyybuUJ+W2wmRyRP8SMNrMRphZT2AWsCy+gbuPcPfh7j4c+DnwRRX5srcMuDq4\nQuN8YI+7d/hqL2VJua0wnR7Ru3uzmd0IPAFUAUvdfZ2ZXR/MX5znGCUPZs+ezcqVK9m1axfAWWY2\nD+gBbTldDswANgEHgWuLFatkpzW3QC8z2wrcjnJb0UpmhKnhCx8vShyVKJ8jTJnVORzLqzpji9cZ\nG/YIU4m57Wz7HZfPbblc1lfK2+lqZ2xZjzAlIiL5oUIvIhJxKvQiIhGnQi8iEnEq9CIiEadCLyIS\ncSr0IiIRp0IvIhJxKvQiIhGnQi8iEnEq9CIiEadCLyIScSr0IiIRp0IvIhJxKvQiIhGnQl/B6uvr\nGTNmDMA4M1uYON/MLjSzPWa2Ovi5rfBRSraUV0mkQl+hWlpaWLBgAStWrABYB8w2s7FJmj7r7ucE\nP3cUNkrJlvIqyajQV6hVq1YxatQoRo4cCeDAI8DM4kYlXaW8SjIq9BVq27ZtDBs2LH7SVqAmSdML\nzGyNma0wszOTrcvM5ptZg5k11NKIY20/uYhfPtN1pVzG0vyEHEOq7aRaV9r4cojNsVDzGvuVkuc2\n231QyP2QkxzizlXYn9VMdDo4uFS0l4Fad99vZjOAx4DRiY3cfQmwBKDOrDiDEEs2MsorKLdRoSP6\nClVTU8OWLVviJw0FtsVPcPe97r4/eL0c6GFmAwsXpWRLeZVkMir0ZjbdzDaY2aYUvfh/FXwNXGtm\nvzOzs8MPVcI0ceJENm7cyObNmwEMmAUsi29jZqeYxb43mtkkYp+X9wodq2ROeZVkOj11Y2ZVwH3A\nJ4md73vJzJa5+/q4ZpuBj7v7+2Z2KbGvepPzEbCEo3v37tx7771MmzYN4EzgTndfZ2bXA7j7YuAq\n4AYzawY+BGa5u76+lzDlVZKxzvJrZh8DFrn7tOD93wC4+z+kaD8AeNXdk3UAtamrq/OGhoa298MX\nPp5d5JKzN++6rN17M2t097ow1l1n5g3xE1J8vtL1L6Xs6Er3Wc2hw8pIvb5UMeSyTE5y/V3jlgsz\nr9A+t6n2Q6j7AHLaD2lzlGpW2J+fdGU1l87VLuY1k1M3NUD8Sb9Uvfit5gErks2I78F/9913M49S\nRERyFmpnrJldRKzQfz3ZfHdf4u517l43aNCgMDctIiIpZFLotwHxF+Z26MUHMLOzgAeAme5etI6d\nU07oXaxNi4iUpEwK/UvAaDMbYWY9Sd6LXwv8Evicu78WfpiZe+HmTzBj/CnFDEFEpKR0etWNuzeb\n2Y3AE0AVsDRJL/5twEnAD4KrtprD7ASS8tE4BOwLx97ncimHLUo+PW3/Vopl0kqzTMr15bJMDnL9\nXfN56Uy73KaIIcx9ADnuhxxiCPvzE/a2uprXjO6MDW6qWJ4wbXHc6+uA67oYS2jG1ZzI8rXvFDsM\nEZGSUDJ3xvbtWZVy3rIb/6zDtOumjEjZ/vqpp3Lr5e0f2Dd0wHFtr1/7u0t5/ZszePavL8oh0piJ\nwweknPfbr36cPz/7IwDMuWA4P71uMl/+xGgW/+8JbW3uvnI8b3xzRs7bv+q8oTz8+fPbTbvxolF8\npm4oq2/7JPPi9s9xPar4X+fW8Oj1H2PFl/9HztsUkfJUMs+6uWPmOL766Ctt73tUGU0tsS8sZw3t\nz6qbP8Hh5qMMq+7T1uaB5za3vX72ry/icPNRALp1M+ZNGcFfTa5lyt1Ps2v/YX587UQu/s4zAPTs\n3v7vW9+eVXx2Yi23Xn4Guw8c4bieVRxpPkr/Pj2ZveQFfv9G+77l6z9+Kl+bNoZTb273JafNyEHH\nc8/sc7ln9rlt0y4YFbvD/LLxQ3h87Xb69upOt24dr6eNv8b96FHn7d0HGT6wb9u01vsN/u+nYzcf\nH9ejig+bWnjl9ks48bgebe1uvXwsN10yhg079nHOsP5J4xSRylAyR/RXnjeUGy48te39yIHHA9Ba\nCwef0LtdkU80rLoPowYf325a7x5VnHhc53/LBvTtyW2fGouZcdLxvejTszv9+/QEwJOcHTODqiRF\nOhd90nyT6dbN2hX5dHpUdYznuJ5VKvIiUjqFHuCkvj3bXk8aUR36+oecmP2ll/F3uKUryuNrTswl\npLDvIxQR6aBkTt1A7Ai8VeuR7OB+qYtzv97d2XeoucORfLxrLhjObb9ax+ATevP0TRfScvRY5R4Q\n/GH5y8m1ncb26fOGcuV5Q5m15IW2P0KN37iYox77A2UGn3+wge17DqVdz+xJtTy+djsTalOf48/G\n7Em1LH1+Mz2qSupvtoiUkJIq9J+dOIwPDh5h7EdO4MLTBlPdtwcTh6c+sp925in8vHErjy3o2Fnb\n6uqPDefqjw1POu/4Xt07PPcllSsm1HD+yJNYu+gS+vWOnQs/6fhe7do8cM3ETtczZfTAdtu8/VNn\nctuyV/nX687njCH9Mool3jcuO4OvXzpGhV5EUiqpQt+jqhs3/s9j4x9cce7QtO2/ecV4vnLxaI7v\nlb9fo/X434KTLK1FPiyfmTiMz0wc1nnDFLp1M3p1S31KSUSkrA8De3bvxtABqTtowxTCaF4iIkVR\n1oW+EL42bQynnNCbcTl2toqIFFtJnbopRROHV/PCzZ8odhhtnv3ri3jvwJFihyEiZURH9GVmWHWf\n0K6Nr6+vZ8yYMQDjUgwRaWZ2TzCE5Bozm9BxLVJqlFdJpEJfoVpaWliwYAErVqwAWAfMNrOxCc0u\nBUYHP/OB+wsbpWRLeZVkVOgr1KpVqxg1ahQjR46E2MVFjwAzE5rNBB70mBeA/mY2pMChShaUV0mm\n0zFj87Zhs3eBtxImDwR2FSGcUosB8h/HAOAEYjn4KPB/gMnufmNrAzP7T+Aud38ueP8b4Ovu7YeF\nNbP5xI4MAcYBr+Yx7kyUQg6LFUN8XscAXyTHvAbzSim3lZzXeGPcPaubborWGevuHcYSNLOGYj/H\nvhRiKEQcZnYVMD14xDRm9rlc1+XuS4AlwXqKvv8qOYb4vJpZh8KdrVLKbbG3X0oxZLuMTt1UrkyG\niMxoGEkpKcqrdKBCX7k6HSIyeH91cJXG+cAed99e6EAlK215JfbMPOVVSu46+iXFDoDSiAHyHEeG\nQ0QuB2YAm4CDwLUZrLoU9l/FxpCQ1/7AP4WUVyj+fi329qFMYyhaZ6yIiBSGTt2IiEScCr2ISMSV\nRKE3s+lmtiG4JbvDLds5rG+YmT1tZuvNbJ2ZfTmYvsjMtpnZ6uBnRtwyfxNsf4OZTYubfp6ZrQ3m\n3WMWe46lmfUys38Lpr9oZsNTxPJmsPzq1suizKzazJ40s43BvwPi2ucljkIIO485xtBhfxdgm0vN\nbKeZvRo3LWWOCxhDys97lutWXo9NK8+8untRf4h1BL4OjAR6Aq8AY7u4ziHAhOB1P+A1YCywCLgp\nSfuxwXZ7ASOCeKqCeauA84ldwbACuDSY/kVgcfB6FvBvKWJ5ExiYMO1bwMLg9ULg7nzHUY55zDGO\nDvu7ANucCkwAXu0sxwWOIennXXmtvLyWwhH9JGCTu7/h7kdIfst2Vtx9u7u/HLzeB/wBqEmzyEzg\nEXc/7O6biV2NMMlit4Wf4O4veGwPPwj8RdwyPwle/xz4ROtRdgbil/1JwjoLGUeYQs9juXD3Z4Dd\nCZNT5biQMYRBeW2vLPNaCoW+BtgS934r6YtyVoJTGecCLwaTvmSxJ/YtjfvalSqGmuB1stjalnH3\nZmAPcFKSEBx4yswaLXY7OcDJfuy65XeAkwsQR77lNY9ZSLa/iyFVjgst2ec9G8pre2WZ11Io9Hlj\nZscDvwC+4u57iT2lbyRwDrAd+McChDHF3c8h9sTABWY2NX5mcISua1zDk3Z/F0MRc1yMz3u+KK/H\nZJ3XUij0ebkd28x6ECvy/+ruvwRw9x3u3uLuR4EfEvtami6GbcHrZLG1LWNm3YETgfcS43D3bcG/\nO4F/D7a5IzgdQ/DvznzHUQAlcVt9iv1dDKlyXDBpPu/ZUF7bK8u8lkKhz+RW/KwE56h/BPzB3b8T\nNz3+UaxXcOxJfMuAWcEVLCOIPad7VfAVba+ZnR+s82rgV3HLXBO8vgr4bfAXPj6OvmbWr/U1cEmw\nzfhlr0lYZ+hxFEjoecxWmv1dDKlyXDBpPu/ZUF7bK8+8FrIXO03P8gxiV8a8DtwSwvqmEPtKtQZY\nHfzMAB4C1gbTlwFD4pa5Jdj+BoIrWoLpdcGOfB24l2N3E/cGHiXWYboKGJkkjpHErlJ4hdggELcE\n008CfgNsBJ4CqvMZR7nmMYftJ93fBdjuw8S+QjcRO4c9L12OCxhDys+78lpZedUjEEREIq7TUzeW\n4uajhDZmGoOyrCiv0aS8SjKZPL2yGfiqu78cnCdrNLMn3X19XJv4MSgnE+sVnhx6tBIm5TWalFfp\noNMjes/s5iONQVlmlNdoUl4lmayeR5/k5qNWqW6qaDeYgcWNP9m3b9/zTj/99OyilbxobGzcDRxA\neY2UruYVlNtS1NjYuMuTDMWaTsaFPsnNR1nzuPEn6+rqvKGhIM8mkjT2799Pv379egOfV16jI4y8\ngnJbiszsrWyXyeg6+mQ3HyUoiZsqJDtNTU1ceeWVALuV1+hQXiVRJlfdJL35KIHGoCwz7s68efM4\n44wzAHakaKa8lhnlVZLJ5NTNnwGfA9aa2epg2s1ALXR5DEopkueff56HHnqI8ePHA4wNcqu8ljnl\nVZLptNC7+3PEnoGero0DC8IKSvJvypQprXfeYWbr3b0usY3yWn6UV0mmFJ51IyIieaRCLyIScSr0\nIiIRp0IvIhJxKvQiIhGnQi8iEnEq9CIiEadCLyIScSr0IiIRp0IvIhJxKvQiIhGnQi8iEnEq9CIi\nEadCLyIScSr0IiIRp0IvIhJxmQwluNTMdprZqynmX2hme8xsdfBzW/hhStjmzp3L4MGDGTduXNL5\nymv5as0tcGay+cpt5cnkiP7HwPRO2jzr7ucEP3d0PSzJtzlz5lBfX99ZM+W1DCm3kqjTQu/uzwC7\nCxCLFNDUqVOprq4udhiSB8qtJArrHP0FZrbGzFaYWdKviwBmNt/MGsys4d133w1p05JHymt0KbcV\nJIxC/zJQ6+5nAd8HHkvV0N2XuHudu9cNGjQohE1LHimv0aXcVpguF3p33+vu+4PXy4EeZjawy5FJ\nUSmv0aXcVp4uF3ozO8XMLHg9KVjne11drxSX8hpdym3l6d5ZAzN7GLgQGGhmW4HbgR4A7r4YuAq4\nwcyagQ+BWe7ueYtYQjF79mxWrlzJrl27AM4ys3kor5HQmlugl/7PCoAVK791dXXe0NBQlG1Le2bW\n6O51YaxLeS0dYeYVlNtSkUtedWesiEjEqdCLiEScCr2ISMSp0IuIRJwKvYhIxKnQi4hEnAq9iEjE\nqdCLiEScCr2ISMSp0IuIRJwKvYhIxKnQi4hEnAq9iEjEqdCLiEScCr2ISMR1WujNbKmZ7TSzV1PM\nNzO7x8w2BYMNTwg/TAnb3LlzGTx4MOPGjUs6X3ktX625BZIO+q3cVp5Mjuh/DExPM/9SYHTwMx+4\nv+thSb7NmTOH+vr6dE2U1zKl3EqiTgu9uz8D7E7TZCbwoMe8APQ3syFhBSj5MXXqVKqrq9M1UV7L\nlHIriTodMzYDNcCWuPdbg2nbExua2XxiRxDU1tZmtPLYEMbRkmr0xlx+1zyOBBlKXlP9TuniDnM/\n5Pr5yWW/Fvt3zUJOuYXatnhz2d9hf1Zz2d/ZrquQ8jmqa0E7Y919ibvXuXvdoEGDCrlpySPlNbri\ncwvKbbkKo9BvA4bFvR8aTJPyprxGl3JbYcIo9MuAq4Oe/POBPe7e4SuglB3lNbqU2wrT6Tl6M3sY\nuBAYaGZbgduBHgDuvhhYDswANgEHgWvzFayEZ/bs2axcuZJdu3YBnGVm81BeI6E1t0Av/Z8VAPN8\n9gCkUVdX5w0NDZ22K4VOkrCVWmesmTXGzsF2XWJei91BWcmdsWHmNba+OoeGtDGoMzZ3+cyr7owV\nEYk4FXoRkYhToRcRiTgVehGRiFOhFxGJOBV6EZGIU6EXEYk4FXoRkYhToRcRiTgVehGRiFOhFxGJ\nOBV6EZGIU6EXEYk4FXoRkYhToRcRiTgVehGRiMuo0JvZdDPbYGabzGxhkvkXmtkeM1sd/NwWfqgS\ntvr6esaMGQMwTnmNDuVVEmUylGAVcB/wSWAr8JKZLXP39QlNn3X3y/MQo+RBS0sLCxYs4Mknn+TU\nU09dB8xWXsuf8irJZHJEPwnY5O5vuPsR4BFgZn7DknxbtWoVo0aNYuTIkQCO8hoJyqsk0+kRPVAD\nbIl7vxWYnKTdBWa2BtgG3OTu6xIbmNl8YD5AbW1t/IyUG083jKKlnZtqfSUwOGSqsS5zWlkOA5K6\ns23bNoYNGxY/Nfy8kmZ/p01D8t8pfe5yWSb7GHL7rIb9OU29vjDzCgm5Bd5qjStleDkOkJvToL/h\nDUKbbn8XrM6k/VW79ruG1Rn7MlDr7mcB3wceS9bI3Ze4e5271w0aNCikTUseKa/RlFFeISG3BQtP\nwpZJod8GxB8iDA2mtXH3ve6+P3i9HOhhZgNDi1JCV1NTw5Yt8V/UlNcoUF4lmUwK/UvAaDMbYWY9\ngVnAsvgGZnaKWew7lplNCtb7XtjBSngmTpzIxo0b2bx5M8S+NCqvEaC8SjKdnqN392YzuxF4AqgC\nlrr7OjO7Ppi/GLgKuMHMmoEPgVnuXTypJHnVvXt37r33XqZNmwZwJnCn8lr+lFdJxoqV37q6Om9o\naAiiyK3DrGw7Y8OULn9pOmPbN7NGd68LI5x2eU0XQxqp8po2dzl15qUR5vrS5ChlinL5XTusO7y8\nAtSZeUMnbdL9nww7fyk/J7mUtBy2k07odSbul8olr7ozVkQk4lToRUQiToVeRCTiVOhFRCIukztj\n86LxT43Y3wYdFotSt/M083JhIa+v2NLeObwo+2XCltP+TrFMunWl+p1yzXeY68upbzDNdop1eUzj\nELAvdNYqXadmajnt73TzshT2dsKuM13NuY7oRUQiToVeRCTiVOhFRCJOhV5EJOJU6EVEIk6FXkQk\n4lToRUQiToVeRCTiVOhFRCJOhV5EJOJU6EVEIi6jQm9m081sg5ltMrOFSeabmd0TzF9jZhPCD1XC\nVl9fz5gxYwDGKa/RobxKok4LvZlVAfcBlwJjgdlmNjah2aXA6OBnPnB/yHFKyFpaWliwYAErVqwA\nWIfyGgnKqySTyRH9JGCTu7/h7keAR4CZCW1mAg96zAtAfzMbEnKsEqJVq1YxatQoRo4cCbGH4ymv\nEaC8SjINkgu9AAADWUlEQVSZPKa4BtgS934rMDmDNjXA9vhGZjaf2BEEwGEW8WpnG08/8mIO4zIu\navduILAr+5WEqksx2KIcxmSNLTMAOMHM3gLGEGJezazTvHYSYfLJi9Is0X4/dDmvuezXBG0x5LSu\nRalndbK+0PIKuf2fzUWG+yghr8mXyXWY4NRy+GwtCjmC9vtnTLbLF/R59O6+BFgCYGYNYQ5cnItK\njsHMrgKmu/t1ZtbZmM9pKa+lE0OYeYXSym2xt19KMWS7TCanbrYBw+LeDw2mZdtGSovyGk3Kq3SQ\nSaF/CRhtZiPMrCcwC1iW0GYZcHXQm38+sMfdO3wNlJLSlldi302V12hQXqWDTk/duHuzmd0IPAFU\nAUvdfZ2ZXR/MXwwsB2YAm4CDwLUZbHtJzlGHp2JjSMhrf+CflNdQRS2vUPz9WuztQ5nGYO7FGoFS\nREQKQXfGiohEnAq9iEjEFaXQd/ZIhQLF8KaZrTWz1WFchpbhNpea2c7468zNrNrMnjSzjcG/Awq8\n/UVmti3YD6vNbEYX1q+8HptWsLymiSGU3CqvEciruxf0h1iH7uvASKAn8AowtghxvAkMLPA2pwIT\ngFfjpn0LWBi8XgjcXeDtLwJuUl7LN6/5zK3yGo28FuOIPpNHKkSSuz8D7E6YPBP4SfD6J8BfFHj7\nYVFe2ytYXtPEEAbltb2yzGsxCn2q268LzYGnzKwxuM27WE72Y9cwvwOcXIQYvmSxpxgu7cJXUeW1\nvVLIK3Q9t8pre2WZ10rujJ3i7ucQe5LfAjObWuyAPPa9rNDXu95P7Gv5OcSedfKPBd5+2JTXY6KU\nW+X1mKzzWoxCXxK3X7v7tuDfncC/E/uKWgw7LHhyYPDvzkJu3N13uHuLux8Ffkju+0F5ba+oeYXQ\ncqu8tleWeS1Goc/kkQp5ZWZ9zaxf62vgEsjPU/kysAy4Jnh9DfCrQm7c2j+e9gpy3w/Ka3tFzSuE\nllvltb3yzGshe7Hjeo1nAK8R682/pQjbH0ns6oFXiA3OUJAYgIeJfdVqInaucx5wEvAbYCPwFFBd\n4O0/BKwF1hD7EA9RXssrr/nOrfJa/nnVIxBERCKukjtjRUQqggq9iEjEqdCLiEScCr2ISMSp0IuI\nRJwKvYhIxKnQi4hE3P8Hnm3yYGC2MeQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5490f936d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Running the RNN\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    plt.ion()\n",
    "    plt.figure()\n",
    "    plt.show()\n",
    "    loss_list = []\n",
    "    \n",
    "    for epoch_idx in range(num_epochs):\n",
    "        x, y = generateData()\n",
    "        _current_state = np.zeros((batch_size, state_size))\n",
    "        \n",
    "        print(\"New Data, epoch\", epoch_idx)\n",
    "        \n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx + truncated_backprop_length\n",
    "            end_idx = start_idx + truncated_backprop_length\n",
    "            \n",
    "            batchX = x[:, start_idx:end_idx]\n",
    "            batchY = y[:, start_idx:end_idx]\n",
    "            \n",
    "            _total_loss, _train_step, _current_state, _prediction_series = sess.run(\n",
    "                [total_loss, train_step, current_state, predictions_series],\n",
    "                feed_dict = {\n",
    "                    batchX_placeholder: batchX,\n",
    "                    batchY_placeholder: batchY,\n",
    "                    init_state: _current_state\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            loss_list.append(_total_loss)\n",
    "            \n",
    "            if batch_idx%100==0:\n",
    "                print('Step: {}, Loss: {}'.format(batch_idx, _total_loss))\n",
    "                plot(loss_list, _prediction_series, batchX, batchY)\n",
    "plt.ioff()\n",
    "plt.show()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
